{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\clickratio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\clickratio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\Users\\clickratio\\AppData\\Local\\Temp\\ipykernel_5596\\3010027505.py:50: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  vectoriser = pickle.load(file)\n",
      "c:\\Users\\clickratio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.2.post1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\clickratio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.2.post1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\clickratio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator BernoulliNB from version 0.22.2.post1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Negative\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import wx\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#if app crashes, use del app\n",
    "##del app\n",
    "\n",
    "## Establishing Dictionary\n",
    "\n",
    "## Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "## Defining set containing all stopwords in english.\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "              'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "#load vectoriser for text processing\n",
    "def load_models():\n",
    "\n",
    "    # Load the vectoriser.\n",
    "    file = open('D:/vectoriser.pickle', 'rb')\n",
    "    vectoriser = pickle.load(file)\n",
    "    file.close()\n",
    "    # Load the LR Model.\n",
    "    file = open('D:/BNB.pickle', 'rb')\n",
    "    LRmodel = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectoriser, LRmodel\n",
    "\n",
    "#sentiment prediction model loading\n",
    "def predict(vectoriser, model, text):\n",
    "    # Predict the sentiment\n",
    "    textdata = vectoriser.transform(preprocess(text))\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "        \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
    "    return df\n",
    "\n",
    "#preprocess text input\n",
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    \n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Defining regex patterns.\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL',tweet)\n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
    "\n",
    "                    # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)        \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            # Checking if the word is a stopword.\n",
    "            #if word not in stopwordlist:\n",
    "            if len(word)>1:\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText\n",
    "\n",
    "class MyFrame(wx.Frame):    \n",
    "    def __init__(self):\n",
    "        super().__init__(parent=None, title='Sentiment Analysis')\n",
    "        panel = wx.Panel(self)        \n",
    "        my_sizer = wx.BoxSizer(wx.VERTICAL)        \n",
    "        self.text_ctrl = wx.TextCtrl(panel)\n",
    "        my_sizer.Add(self.text_ctrl, 0, wx.ALL | wx.EXPAND, 5)        \n",
    "        my_btn = wx.Button(panel, label='Submit the Post')\n",
    "        my_btn.Bind(wx.EVT_BUTTON, self.on_press)\n",
    "        my_sizer.Add(my_btn, 0, wx.ALL | wx.CENTER, 5)  \n",
    "        self.text_ctrl1 = wx.TextCtrl(panel)\n",
    "        my_sizer.Add(self.text_ctrl1, 0, wx.ALL | wx.EXPAND, 5)                 \n",
    "        panel.SetSizer(my_sizer)        \n",
    "        self.Show()\n",
    "\n",
    "    def on_press(self, event):\n",
    "        value = self.text_ctrl.GetValue()\n",
    "        if not value:\n",
    "            print(\"Try harder by entering something!\")\n",
    "        else:\n",
    "            vectoriser, LRmodel = load_models()\n",
    "            text = [value]\n",
    "            df = predict(vectoriser, LRmodel, text)\n",
    "            self.text_ctrl1.SetValue(df.sentiment.to_string()) \n",
    "            print(df.sentiment)\n",
    "            ##print(df.head())\n",
    "            ##print(f'You typed: \"{value}\"')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = wx.App()\n",
    "    frame = MyFrame()\n",
    "    app.MainLoop()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ae9bcfa8e9b71d081b435403a849f55d4b524f08442fa759938af2199ad1ced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
